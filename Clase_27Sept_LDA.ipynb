{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clase_27Sept_LDA.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPG4X7YWU5Fki/jmLB7Dpn+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"vnkzpvwvd3Ts"},"source":["El análisis discriminante lineal (LDA) es una técnica de reducción de dimensionalidad. Como su nombre lo indica, las técnicas de reducción de la dimensionalidad reducen el número de dimensiones (es decir, variables) en un conjunto de datos mientras retienen la mayor cantidad de información posible.\n","\n","Por ejemplo, suponga que graficamos la relación entre dos variables donde cada color representa una clase diferente.\n","\n","![](https://miro.medium.com/max/700/1*o2TKovc_lkJ9_ISxZxrbog.png)\n","\n","Si quisiéramos reducir el número de dimensiones a 1, un enfoque sería proyectar todo en el eje x.\n","\n","![](https://miro.medium.com/max/700/1*5lugB_AavKEr3ghDGC6hOA.png)\n","\n","![](https://miro.medium.com/max/635/1*Z202fIHoHkW5KhxxXcQ8jA.png)\n","\n","Esto es malo porque ignora cualquier información útil proporcionada por la segunda función. Por otro lado, el Análisis Discriminante Lineal, o LDA, usa la información de ambas características para crear un nuevo eje y proyecta los datos en el nuevo eje de tal manera que minimiza la varianza y maximiza la distancia entre las medias del dos clases.\n","\n","![](https://miro.medium.com/max/700/1*Fz3JQ80No5Nnbap28EGRTg.png)\n","\n","![](https://miro.medium.com/max/700/1*5lhckC2RQzq28zNL7WtU5A.png)\n","\n","![](https://miro.medium.com/max/700/1*W48aQ0LkZ5dm1_uow6FD2w.png)\n","\n","Veamos cómo podemos implementar el análisis discriminante lineal desde cero con Python. Para comenzar, importe las siguientes bibliotecas.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"-29dMTOmcjGq"},"source":["from sklearn.datasets import load_wine\n","import pandas as pd\n","import numpy as np\n","np.set_printoptions(precision=4)\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","sns.set()\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gbrrC7G1eqIh"},"source":["En este notebook , trabajaremos con el conjunto de datos de wine que se puede obtener del repositorio de aprendizaje automático de UCI."]},{"cell_type":"code","metadata":{"id":"8PNjZoXYcmLL"},"source":["wine = load_wine()\n","X = pd.DataFrame(wine.data, columns=wine.feature_names)\n","y = pd.Categorical.from_codes(wine.target, wine.target_names)\n","X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hQcKw5TQe80g"},"source":["Las variables se componen de varias características como el magnesio y el contenido de alcohol del vino."]},{"cell_type":"code","metadata":{"id":"C4q0dnIhcqcT"},"source":["X.head()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yc4G48HIfDEq"},"source":["Hay 3 tipos diferentes de vino."]},{"cell_type":"code","metadata":{"id":"gK2fVFxfcsSd"},"source":["wine.target_names"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lAYZAEo0fLhm"},"source":["Creamos un DataFrame que contiene tanto las características como las clases."]},{"cell_type":"code","metadata":{"id":"j6XiFdQnculx"},"source":["df = X.join(pd.Series(y, name='class'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uV7I0yvifQv4"},"source":["El análisis discriminante lineal se puede dividir en los siguientes pasos:\n","\n","> Calcule las matrices de dispersión intraclase y entre clases\n","\n","> Calcule los autovectores y los autovalores correspondientes para las matrices de dispersión.\n","\n","> Ordene los valores propios y seleccione el k superior\n","\n","> Cree una nueva matriz que contenga vectores propios que se asignen a los k valores propios\n","\n","> Obtenga las nuevas características (es decir, componentes LDA) tomando el producto escalar de los datos y la matriz del paso anterior.\n","\n","Para cada clase, creamos un vector con las medias de cada característica.\n"]},{"cell_type":"code","metadata":{"id":"aBYhaYSxcxLY"},"source":["class_feature_means = pd.DataFrame(columns=wine.target_names)\n","for c, rows in df.groupby('class'):\n","    class_feature_means[c] = rows.mean()\n","class_feature_means"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uyyXPHpefv9b"},"source":["Luego, conectamos los vectores mediosy obtenemos la matriz de dispersión intraclase."]},{"cell_type":"code","metadata":{"id":"Y5yEDvmYcz_q"},"source":["within_class_scatter_matrix = np.zeros((13,13))\n","for c, rows in df.groupby('class'):\n","  rows = rows.drop(['class'], axis=1)\n","    \n","s = np.zeros((13,13))\n","for index, row in rows.iterrows():\n","  x, mc = row.values.reshape(13,1), class_feature_means[c].values.reshape(13,1)\n","  s += (x - mc).dot((x - mc).T)\n","  within_class_scatter_matrix += s"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ujjf_KeHgGcc"},"source":["A continuación, calculamos la matriz de dispersión entre clases : "]},{"cell_type":"code","metadata":{"id":"bj0P_1BqdK8w"},"source":["feature_means = df.mean()\n","between_class_scatter_matrix = np.zeros((13,13))\n","for c in class_feature_means:    \n","    n = len(df.loc[df['class'] == c].index)\n","    mc, m = class_feature_means[c].values.reshape(13,1), feature_means.values.reshape(13,1)\n","    between_class_scatter_matrix += n * (mc - m).dot((mc - m).T)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sNLiYZ4DgNGv"},"source":["Luego, resolvemos el problema de valores propios generalizados para obtener los discriminantes lineales."]},{"cell_type":"code","metadata":{"id":"5NpbOOIydON8"},"source":["eigen_values, eigen_vectors = np.linalg.eig(np.linalg.inv(within_class_scatter_matrix).dot(between_class_scatter_matrix))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7iINoBpHgT7N"},"source":["Los vectores propios con los valores propios más altos contienen la mayor cantidad de información sobre la distribución de los datos. Por lo tanto, ordenamos los autovalores de mayor a menor y seleccionamos los primeros k autovectores. Para asegurarnos de que el valor propio se corresponda con el mismo vector propio después de la clasificación, los colocamos en una matriz temporal."]},{"cell_type":"code","metadata":{"id":"jvplY8WJdQt6"},"source":["pairs = [(np.abs(eigen_values[i]), eigen_vectors[:,i]) for i in range(len(eigen_values))]\n","pairs = sorted(pairs, key=lambda x: x[0], reverse=True)\n","for pair in pairs:\n","    print(pair[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SmW4StcLggon"},"source":["Con solo mirar los valores, es difícil determinar qué parte de la varianza se explica por cada componente. Por tanto, lo expresamos como porcentaje."]},{"cell_type":"code","metadata":{"id":"OkaF4QgzdT0V"},"source":["eigen_value_sums = sum(eigen_values)\n","print('Explained Variance')\n","for i, pair in enumerate(pairs):\n","    print('Eigenvector {}: {}'.format(i, (pair[0]/eigen_value_sums).real))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dl9tuUkPgrkD"},"source":["Primero, creamos una matriz W con los dos primeros autovectores."]},{"cell_type":"code","metadata":{"id":"cmVzGoMDdWnd"},"source":["w_matrix = np.hstack((pairs[0][1].reshape(13,1), pairs[1][1].reshape(13,1))).real"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QCUnAdgIgwky"},"source":["Luego, guardamos el producto escalar de X y W en una nueva matriz Y : Y = XW\n","\n","donde X es una matriz n × d con n muestras y d dimensiones, e Y es una matriz n × k con n muestras y k (k < n) dimensiones. En otras palabras, Y se compone de los componentes LDA, o dicho de otra forma, el nuevo espacio de características."]},{"cell_type":"code","metadata":{"id":"VirJfQgHdYhL"},"source":["X_lda = np.array(X.dot(w_matrix))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xBy37km0g7Cm"},"source":["matplotlib no puede manejar variables categóricas directamente. Por lo tanto, codificamos cada clase como un número para poder incorporar las etiquetas de clase en nuestro gráfico."]},{"cell_type":"code","metadata":{"id":"MFuIhEe5daEB"},"source":["le = LabelEncoder()\n","y = le.fit_transform(df['class'])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l9beFrDQhB24"},"source":["Luego, graficamos los datos en función de los dos componentes LDA y usamos un color diferente para cada clase."]},{"cell_type":"code","metadata":{"id":"-FNth-qfdbtk"},"source":["plt.xlabel('LD1')\n","plt.ylabel('LD2')\n","plt.scatter(\n","    X_lda[:,0],\n","    X_lda[:,1],\n","    c=y,\n","    cmap='rainbow',\n","    alpha=0.7,\n","    edgecolors='b'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yGpNnMxWhJKQ"},"source":["En lugar de implementar el algoritmo de Análisis Discriminante Lineal desde cero cada vez, podemos usar la clase LinearDiscriminantAnalysis predefinida que pone a nuestra disposición la biblioteca scikit-learn."]},{"cell_type":"code","metadata":{"id":"QvbNmIkVdd3q"},"source":["from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","lda = LinearDiscriminantAnalysis()\n","X_lda = lda.fit_transform(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bLRb_xPwhNAf"},"source":["Podemos acceder a la siguiente propiedad para obtener la varianza explicada por cada componente."]},{"cell_type":"code","metadata":{"id":"HaShRhWqdfpR"},"source":["lda.explained_variance_ratio_\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rQa9jkDHhQpu"},"source":["Al igual que antes, graficamos los dos componentes LDA."]},{"cell_type":"code","metadata":{"id":"_FSlLNMPdiL6"},"source":["plt.xlabel('LD1')\n","plt.ylabel('LD2')\n","plt.scatter(\n","    X_lda[:,0],\n","    X_lda[:,1],\n","    c=y,\n","    cmap='rainbow',\n","    alpha=0.7,\n","    edgecolors='b'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O0gTlEk1hYp8"},"source":["---\n","A continuación, echemos un vistazo a cómo se compara LDA con el análisis de componentes principales o PCA. Comenzamos creando y ajustando una instancia de la clase PCA."]},{"cell_type":"code","metadata":{"id":"UDtsZMscdpmv"},"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ggpP9J7Ohdfz"},"source":["Podemos acceder a la propiedad explained_variance_ratio_  para ver el porcentaje de la varianza explicada por cada componente."]},{"cell_type":"code","metadata":{"id":"-S7JEcjZdrL4"},"source":["pca.explained_variance_ratio_\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2Hdu_aNdhkfw"},"source":["Como podemos ver, PCA seleccionó los componentes que resultarían en la mayor dispersión (retener la mayor cantidad de información) y no necesariamente los que maximizan la separación entre clases."]},{"cell_type":"code","metadata":{"id":"N4U8mF3GdtZk"},"source":["plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.scatter(\n","    X_pca[:,0],\n","    X_pca[:,1],\n","    c=y,\n","    cmap='rainbow',\n","    alpha=0.7,\n","    edgecolors='b'\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZyGWUeZqhsCV"},"source":["A continuación, veamos si podemos crear un modelo para clasificar el uso de los componentes LDA como características. Primero, dividimos los datos en conjuntos de entrenamiento y prueba."]},{"cell_type":"code","metadata":{"id":"Bf8EtOszdvPl"},"source":["X_train, X_test, y_train, y_test = train_test_split(X_lda, y, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VbjPj0dHhw0a"},"source":["Luego, construimos y entrenamos un árbol de decisiones. Después de predecir la categoría de cada muestra en el conjunto de prueba, creamos una matriz de confusión para evaluar el desempeño del modelo."]},{"cell_type":"code","metadata":{"id":"ENIZ49Rddw2L"},"source":["dt = DecisionTreeClassifier()\n","dt.fit(X_train, y_train)\n","y_pred = dt.predict(X_test)\n","confusion_matrix(y_test, y_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"duPBLQs7h2pr"},"source":["Como podemos ver, el clasificador Decision Tree clasificó correctamente todo en el conjunto de prueba."]}]}